{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO4jFmXJvGvH7krNPL8TpeD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f"],"metadata":{"id":"4tdnCqp3pJF9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"iuzBOJ0zo7-f"},"outputs":[],"source":["!pip install yfinance\n","!pip install yahoofinancials"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","# import tensorflow as tf\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()\n","from IPython.utils.text import long_substr\n","\n","from collections import deque\n","import random\n","\n","import yfinance as yf\n","from yahoofinancials import YahooFinancials\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"3OlHnyoEtJH2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"nV4dXQrRtLCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataGoogle = yf.download(\"GOOG\", start=\"2016-01-01\", end=\"2017-12-31\")\n","dataGoogle['Date'] = dataGoogle.index\n","dataGoogle = dataGoogle[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n","dataGoogle = dataGoogle.reset_index(drop=True)\n","dataGoogle['Date'] = pd.to_datetime(dataGoogle['Date']).apply(lambda x: x.date())\n","dataGoogle_train = dataGoogle\n","dataGoogle_train"],"metadata":{"id":"J20x3zRqtM3k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataGoogle2 = yf.download(\"GOOG\", start=\"2018-01-01\", end=\"2018-05-31\")\n","dataGoogle2['Date'] = dataGoogle2.index\n","dataGoogle2 = dataGoogle2[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n","dataGoogle2 = dataGoogle2.reset_index(drop=True)\n","dataGoogle2['Date'] = pd.to_datetime(dataGoogle2['Date']).apply(lambda x: x.date())\n","dataGoogle_test = dataGoogle2\n","dataGoogle_test"],"metadata":{"id":"pR2K5H1NtO-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataGoogle_train.to_csv('/content/drive/MyDrive/Fall 2022/Reinforcement Learning/Project/dataGoogle_train_2y.csv')\n","dataGoogle_test.to_csv('/content/drive/MyDrive/Fall 2022/Reinforcement Learning/Project/dataGoogle_test_1y.csv')"],"metadata":{"id":"52C0lK8otQ4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/Fall 2022/Reinforcement Learning/Project/dataGoogle_train_AC.csv')\n","df = df.drop(['Unnamed: 0'], axis=1)\n","df.head()"],"metadata":{"id":"EASrZWWBtS47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test = pd.read_csv('/content/drive/MyDrive/Fall 2022/Reinforcement Learning/Project/dataGoogle_test_1y.csv')\n","df_test = df_test.drop(['Unnamed: 0'], axis=1)\n","df_test.head()"],"metadata":{"id":"TbzYiB1UtUkc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Agent"],"metadata":{"id":"_3TR0pMAtXiG"}},{"cell_type":"code","source":["class Actor:\n","    def __init__(self, name, input_size, output_size, size_layer):\n","        with tf.variable_scope(name):\n","            self.X = tf.placeholder(tf.float32, (None, None, input_size)) #Input\n","            self.hidden_layer = tf.placeholder(tf.float32, (None, 2 * size_layer)) #Middle_layer\n","            cell = tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False) #LSTM\n","            self.rnn,self.last_state = tf.nn.dynamic_rnn(inputs=self.X, cell=cell, dtype=tf.float32, initial_state=self.hidden_layer) #RNN\n","            self.logits = tf.layers.dense(self.rnn[:,-1], output_size) #Output_Forward"],"metadata":{"id":"ASRNcarb3CCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Critic:\n","    def __init__(self, name, input_size, output_size, size_layer, learning_rate):\n","        with tf.variable_scope(name):\n","            self.X = tf.placeholder(tf.float32, (None, None, input_size)) #Input\n","            self.Y = tf.placeholder(tf.float32, (None, output_size)) #Output\n","            self.hidden_layer = tf.placeholder(tf.float32, (None, 2 * size_layer)) #Hidden_layer\n","            self.REWARD = tf.placeholder(tf.float32, (None, 1)) #Reward\n","            feed_critic = tf.layers.dense(self.X, size_layer, activation = tf.nn.relu) #Middle_layer1\n","            cell = tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False) #LSTM\n","            self.rnn,self.last_state = tf.nn.dynamic_rnn(inputs=self.X, cell=cell, dtype=tf.float32, initial_state=self.hidden_layer) #RNN\n","            feed_critic = tf.layers.dense(self.rnn[:,-1], output_size, activation = tf.nn.relu) + self.Y #Middle_layer2\n","            feed_critic = tf.layers.dense(feed_critic, size_layer//2, activation = tf.nn.relu) #Middle_layer3\n","            self.logits = tf.layers.dense(feed_critic, 1) #Output_Forward\n","            self.loss = tf.reduce_mean(tf.square(self.REWARD - self.logits)) #Average_loss\n","            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss) #Optimizer"],"metadata":{"id":"A0h63NGh3Tfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Agent:\n","\n","    LEARNING_RATE = 0.001\n","    BATCH_SIZE = 32\n","    LAYER_SIZE = 256\n","    OUTPUT_SIZE = 3\n","    EPSILON = 0.5\n","    DECAY_RATE = 0.005\n","    MIN_EPSILON = 0.1\n","    GAMMA = 0.99\n","    MEMORIES = deque()\n","    MEMORY_SIZE = 300\n","    COPY = 1000\n","    T_COPY = 0\n","\n","    def __init__(self, state_size, window_size, trend, test_data):\n","        self.state_size = state_size\n","        self.window_size = window_size\n","        self.half_window = window_size // 2\n","        self.trend = trend\n","        self.test_data = test_data\n","        self.INITIAL_FEATURES = np.zeros((4, self.state_size))\n","\n","      # Neural Network Model\n","        tf.reset_default_graph()\n","        self.actor = Actor('actor-original', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE)\n","        self.actor_target = Actor('actor-target', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE)\n","        self.critic = Critic('critic-original', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE)\n","        self.critic_target = Critic('critic-target', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE)\n","        self.grad_critic = tf.gradients(self.critic.logits, self.critic.Y)\n","        self.actor_critic_grad = tf.placeholder(tf.float32, [None, self.OUTPUT_SIZE])\n","        weights_actor = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n","        self.grad_actor = tf.gradients(self.actor.logits, weights_actor, -self.actor_critic_grad)\n","        grads = zip(self.grad_actor, weights_actor)\n","        self.optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE).apply_gradients(grads)\n","        self.sess = tf.InteractiveSession()\n","        self.sess.run(tf.global_variables_initializer())\n","    \n","    def _assign(self, from_name, to_name):\n","        from_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=from_name)\n","        to_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=to_name)\n","        for i in range(len(from_w)):\n","            assign_op = to_w[i].assign(from_w[i])\n","            self.sess.run(assign_op)\n","            \n","    def _memorize(self, state, action, reward, new_state, done, rnn_state):\n","        self.MEMORIES.append((state, action, reward, new_state, done, rnn_state))\n","        if len(self.MEMORIES) > self.MEMORY_SIZE:\n","            self.MEMORIES.popleft()\n","\n","    def act(self, state):\n","        if np.random.rand() < self.EPSILON:\n","            action = np.random.randint(self.OUTPUT_SIZE) #Random\n","        else:\n","            prediction = self.sess.run(self.actor.logits, feed_dict={self.actor.X:[state]})[0]\n","            action = np.argmax(prediction) #Greedy\n","        return action\n","    \n","    def replay(self, replay1):\n","        states = np.array([a[0] for a in replay1])\n","        new_states = np.array([a[3] for a in replay1])\n","        init_values = np.array([a[-1] for a in replay1])\n","        Q = self.sess.run(self.actor.logits, feed_dict={self.actor.X: states, self.actor.hidden_layer: init_values})\n","        Q_target = self.sess.run(self.actor_target.logits, feed_dict={self.actor_target.X: states, self.actor_target.hidden_layer: init_values})\n","        grads = self.sess.run(self.grad_critic, feed_dict={self.critic.X:states, self.critic.Y:Q, self.critic.hidden_layer: init_values})[0]\n","        self.sess.run(self.optimizer, feed_dict={self.actor.X:states, self.actor_critic_grad:grads, self.actor.hidden_layer: init_values})\n","        \n","        rewards = np.array([a[2] for a in replay1]).reshape((-1, 1))\n","        rewards_target = self.sess.run(self.critic_target.logits, feed_dict={self.critic_target.X:new_states,self.critic_target.Y:Q_target, self.critic_target.hidden_layer: init_values})\n","        \n","        for i in range(len(replay1)):\n","            if not replay1[0][-2]:\n","                rewards[i] += self.GAMMA * rewards_target[i]\n","        loss, _ = self.sess.run([self.critic.loss, self.critic.optimizer], \n","                                feed_dict={self.critic.X:states, self.critic.Y:Q, self.critic.REWARD:rewards,\n","                                          self.critic.hidden_layer: init_values})\n","        return loss\n","    \n","    def get_state(self, t):\n","        window_size = self.window_size + 1\n","        d = t - window_size + 1\n","        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n","        res = []\n","        for i in range(window_size - 1):\n","            res.append(block[i + 1] - block[i])\n","        return np.array(res)\n","\n","\n","    def train(self, iterations, checkpoint, initial_money):\n","        lossV = []\n","        profitV = []\n","        profitIter = []\n","        for i in range(iterations):\n","            loss_temp = []\n","            profit_temp = []\n","            total_profit = 0\n","            inventory = []\n","            state = self.get_state(0)\n","            starting_money = initial_money\n","            init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n","            for k in range(self.INITIAL_FEATURES.shape[0]):\n","                self.INITIAL_FEATURES[k,:] = state\n","            for t in range(0, len(self.trend) - 1):\n","                if (self.T_COPY + 1) % self.COPY == 0:\n","                    self._assign('actor-original', 'actor-target')\n","                    self._assign('critic-original', 'critic-target')\n","                    \n","                if np.random.rand() < self.EPSILON:\n","                    action = np.random.randint(self.OUTPUT_SIZE)\n","                else:\n","                    action, last_state = self.sess.run([self.actor.logits,\n","                                                  self.actor.last_state],\n","                                                  feed_dict={self.actor.X:[self.INITIAL_FEATURES],\n","                                                             self.actor.hidden_layer:init_value})\n","                    action, init_value = np.argmax(action[0]), last_state\n","                \n","                next_state = self.get_state(t + 1)\n","                \n","              #Buy\n","                if action == 1 and starting_money >= self.trend[t]:\n","                    inventory.append(self.trend[t])\n","                    starting_money -= self.trend[t]\n","                \n","              #Sell\n","                elif action == 2 and len(inventory) > 0:\n","                    bought_price = inventory.pop(0)\n","                    total_profit += self.trend[t] - bought_price\n","                    starting_money += self.trend[t]\n","                    \n","              #Reward\n","                reward = ((starting_money - initial_money) / initial_money)\n","                new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n","\n","                self._memorize(self.INITIAL_FEATURES, action, reward, new_state, \n","                               starting_money < initial_money, init_value[0])\n","                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n","                self.INITIAL_FEATURES = new_state\n","                replay1 = random.sample(self.MEMORIES, batch_size)\n","                loss = self.replay(replay1)\n","                loss_temp.append(loss)\n","                profit_temp.append(total_profit)\n","                self.T_COPY += 1\n","                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n","            if (i+1) % checkpoint == 0:\n","                print('epoch: %d, total profit: %f.3, loss: %f, total money: %f'%(i + 1, total_profit, loss, starting_money))\n","                profitIter.append(total_profit)\n","            lossV.append(np.mean(loss_temp))\n","            profitV.append(np.mean(profit_temp))\n","        return lossV, profitV, profitIter\n","\n","\n","    def evaluate(self, initial_money):\n","        starting_money = initial_money\n","        states_sell = []\n","        states_buy = []\n","        inventory = []\n","        state = self.get_state(0)\n","        init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n","        for k in range(self.INITIAL_FEATURES.shape[0]):\n","            self.INITIAL_FEATURES[k,:] = state\n","        for t in range(0, len(self.test_data) - 1):\n","            \n","            if np.random.rand() < self.EPSILON:\n","                action = np.random.randint(self.OUTPUT_SIZE)\n","            else:\n","                action, last_state = self.sess.run([self.actor.logits,\n","                                                  self.actor.last_state],\n","                                                  feed_dict={self.actor.X:[self.INITIAL_FEATURES],\n","                                                             self.actor.hidden_layer:init_value})\n","                action, init_value = np.argmax(action[0]), last_state\n","                    \n","            next_state = self.get_state(t + 1)\n","            \n","          #Buy\n","            if action == 1 and initial_money >= self.test_data[t]:\n","                inventory.append(self.test_data[t])\n","                initial_money -= self.test_data[t]\n","                states_buy.append(t)\n","                # print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.test_data[t], initial_money))\n","            \n","          #Sell\n","            elif action == 2 and len(inventory):\n","                bought_price = inventory.pop(0)\n","                initial_money += self.test_data[t]\n","                states_sell.append(t)\n","                try:\n","                    reward = ((close[t] - bought_price) / bought_price) * 100\n","                except:\n","                    reward = 0\n","                # print('day %d, sell 1 unit at price %f, investment %f %%, total balance %f,' % (t, close[t], reward, initial_money))\n","            \n","            new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n","            self.INITIAL_FEATURES = new_state\n","        reward = ((initial_money - starting_money) / starting_money) * 100\n","        total_gains = initial_money - starting_money\n","        return states_buy, states_sell, total_gains, reward"],"metadata":{"id":"jAQiiWcItdng"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["close = df.Close.values.tolist()\n","close_test = df_test.Close.values.tolist()\n","initial_money = 10000\n","window_size = 5\n","agent = Agent(state_size = window_size, \n","              window_size = window_size, \n","              trend = close, \n","              test_data = close_test)\n","lossV, profitV, profitIter = agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"],"metadata":{"id":"YRWXQT-VtwMq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize = (20,10))\n","plt.plot(lossV, color='r')\n","plt.title('Loss during Training')\n","plt.xlabel('Iterations')\n","plt.ylabel('Loss')\n","plt.show()"],"metadata":{"id":"XPkUqG-FtyJc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize = (20,10))\n","plt.plot(profitV, color='r')\n","plt.title('Profit during Training (Final Profit: %f)'%(profitV[len(profitV)-1]))\n","plt.xlabel('Iterations')\n","plt.ylabel('Profit')\n","plt.show()"],"metadata":{"id":"oDRc3nZot1D-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize = (20,10))\n","plt.plot(profitIter, color='r')\n","plt.title('Profit during Training (Final Profit: %f)'%(profitIter[len(profitIter)-1]))\n","plt.xlabel('Iterations')\n","plt.ylabel('Profit')\n","plt.show()"],"metadata":{"id":"KDBJCNHHt2n1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Test"],"metadata":{"id":"i6T8Bb-ct4js"}},{"cell_type":"code","source":["states_buy, states_sell, total_gains, reward = agent.evaluate(initial_money = initial_money)"],"metadata":{"id":"Dz9QTxxnt6FA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize = (20,10))\n","plt.plot(close_test, color='r', lw=2.)\n","plt.plot(close_test, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n","plt.plot(close_test, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n","plt.title('Testing Close Price (Total Profit: %f)'%(total_gains))\n","plt.legend()\n","plt.show()"],"metadata":{"id":"U6Uvbmzmt8Ru"},"execution_count":null,"outputs":[]}]}