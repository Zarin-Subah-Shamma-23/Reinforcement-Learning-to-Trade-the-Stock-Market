{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1atBbSZwQvYIdZmvEWlzGmGTCODnYMGcy","timestamp":1667537286643},{"file_id":"1QSAOuEXZyQaXsLRuGIrwsWYXYCE2rLQj","timestamp":1667515932131}],"authorship_tag":"ABX9TyP0ZG/Q6sXKme7vwopxwy8e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install yfinance\n","!pip install yahoofinancials"],"metadata":{"id":"a80kzM7zklo9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKCk2qZkhCkM"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","# import tensorflow as tf\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()\n","\n","from collections import deque\n","import random\n","\n","import yfinance as yf\n","from yahoofinancials import YahooFinancials\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Wfi1QvgohLcO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataGoogle = yf.download(\"GOOG\", start=\"2016-01-01\", end=\"2017-12-31\")\n","dataGoogle['Date'] = dataGoogle.index\n","dataGoogle = dataGoogle[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n","dataGoogle = dataGoogle.reset_index(drop=True)\n","dataGoogle['Date'] = pd.to_datetime(dataGoogle['Date']).apply(lambda x: x.date())\n","dataGoogle_train = dataGoogle\n","dataGoogle_train"],"metadata":{"id":"0kn_WJpej9eX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataGoogle2 = yf.download(\"GOOG\", start=\"2018-01-01\", end=\"2018-05-31\")\n","dataGoogle2['Date'] = dataGoogle2.index\n","dataGoogle2 = dataGoogle2[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n","dataGoogle2 = dataGoogle2.reset_index(drop=True)\n","dataGoogle2['Date'] = pd.to_datetime(dataGoogle2['Date']).apply(lambda x: x.date())\n","dataGoogle_test = dataGoogle2\n","dataGoogle_test"],"metadata":{"id":"Gnrgbutih4xs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataGoogle_train.to_csv('/content/drive/MyDrive/Fall 2022/Reinforcement Learning/Project/dataGoogle_train_2y.csv')\n","dataGoogle_test.to_csv('/content/drive/MyDrive/Fall 2022/Reinforcement Learning/Project/dataGoogle_test_1y.csv')"],"metadata":{"id":"oex5DfX5j_dV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/Fall 2022/Reinforcement Learning/Project/dataGoogle_train_2y.csv')\n","df = df.drop(['Unnamed: 0'], axis=1)\n","df.head()"],"metadata":{"id":"bFBPgHexhW1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test = pd.read_csv('/content/drive/MyDrive/Fall 2022/Reinforcement Learning/Project/dataGoogle_test_1y.csv')\n","df_test = df_test.drop(['Unnamed: 0'], axis=1)\n","df_test.head()"],"metadata":{"id":"eu9RRyadiCPQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Agent"],"metadata":{"id":"4ZbhjI4usnMZ"}},{"cell_type":"code","source":["class Agent:\n","    def __init__(self, state_size, window_size, trend, batch_size, test_data):\n","        self.state_size = state_size\n","        self.window_size = window_size\n","        self.half_window = window_size // 2\n","        self.trend = trend #Close_price\n","        self.test_data = test_data\n","        self.action_size = 3 #Buy, Sell, Hold\n","        self.batch_size = batch_size\n","        self.memory = deque(maxlen = 1000)\n","        self.inventory = []\n","\n","        self.gamma = 0.95\n","        self.epsilon = 0.5\n","        self.epsilon_min = 0.01\n","        self.epsilon_decay = 0.999\n","\n","      # Neural Network Model\n","        tf.reset_default_graph()\n","        self.sess = tf.InteractiveSession()\n","        self.X = tf.placeholder(tf.float32, [None, self.state_size]) #Input\n","        self.Y = tf.placeholder(tf.float32, [None, self.action_size]) #Output\n","        feed = tf.layers.dense(self.X, 256, activation = tf.nn.relu) #Middle_layer\n","        self.logits = tf.layers.dense(feed, self.action_size) #Output_Forward\n","        self.loss = tf.reduce_mean(tf.square(self.Y - self.logits)) #Average_loss\n","        self.optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(self.loss) #Optimizer\n","        self.sess.run(tf.global_variables_initializer())\n","\n","    def act(self, state):\n","        if random.random() <= self.epsilon:\n","            return random.randrange(self.action_size) #Random\n","        return np.argmax(\n","            self.sess.run(self.logits, feed_dict = {self.X: state})[0] #Greedy\n","        )\n","    \n","    def get_state(self, t):\n","        window_size = self.window_size + 1\n","        d = t - window_size + 1\n","        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n","        res = []\n","        for i in range(window_size - 1):\n","            res.append(block[i + 1] - block[i])\n","        return np.array([res])\n","\n","    def replay(self, batch_size):\n","        mini_batch = []\n","        l = len(self.memory)\n","        for i in range(l - batch_size, l):\n","            mini_batch.append(self.memory[i])\n","        replay_size = len(mini_batch) #32\n","        X = np.empty((replay_size, self.state_size)) #(32, 10)\n","        Y = np.empty((replay_size, self.action_size)) #(32, 3)\n","        states = np.array([a[0][0] for a in mini_batch]) #1st_value = state\n","        new_states = np.array([a[3][0] for a in mini_batch]) #4th_value = next_state\n","        Q = self.sess.run(self.logits, feed_dict = {self.X: states}) #Hashing the states into X\n","        Q_new = self.sess.run(self.logits, feed_dict = {self.X: new_states}) #Hashing the new_states into X\n","        for i in range(len(mini_batch)):\n","            state, action, reward, next_state, done = mini_batch[i] # 0:state, 1:action, 2:reward, 3:next_state, 4:done\n","            target = Q[i] \n","            target[action] = reward \n","            if not done:\n","                target[action] += self.gamma * np.amax(Q_new[i]) #Reward calculation using Incremental_implementation\n","            X[i] = state\n","            Y[i] = target\n","        loss, _ = self.sess.run(\n","            [self.loss, self.optimizer], feed_dict = {self.X: X, self.Y: Y}\n","        )\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","        return loss\n","        \n","    def train(self, iterations, checkpoint, initial_money):\n","        lossV = []\n","        profitV = []\n","        profitIter = []\n","        for i in range(iterations):\n","            loss_temp = []\n","            profit_temp = []\n","            total_profit = 0\n","            inventory = []\n","            state = self.get_state(0)\n","            starting_money = initial_money\n","            for t in range(0, len(self.trend) - 1):\n","                action = self.act(state)\n","                next_state = self.get_state(t + 1)\n","                \n","              #Buy\n","                if action == 1 and starting_money >= self.trend[t] and t < (len(self.trend) - self.half_window):\n","                    inventory.append(self.trend[t])\n","                    starting_money -= self.trend[t]\n","                \n","              #Sell\n","                elif action == 2 and len(inventory) > 0:\n","                    bought_price = inventory.pop(0)\n","                    total_profit += self.trend[t] - bought_price\n","                    starting_money += self.trend[t]\n","\n","               #Reward      \n","                reward = ((starting_money - initial_money) / initial_money)\n","                self.memory.append((state, action, reward, \n","                                    next_state, starting_money < initial_money))\n","                \n","                state = next_state\n","                batch_size = min(self.batch_size, len(self.memory))\n","                loss = self.replay(batch_size)\n","                loss_temp.append(loss)\n","                profit_temp.append(total_profit)\n","            if (i+1) % checkpoint == 0:\n","                print('epoch: %d, total profit: %f, loss: %f, total money: %f'%(i + 1, total_profit, loss, starting_money))\n","                profitIter.append(total_profit)\n","            lossV.append(np.mean(loss_temp))\n","            profitV.append(np.mean(profit_temp))\n","        return lossV, profitV, profitIter\n","\n","    def evaluate(self, initial_money):\n","        starting_money = initial_money\n","        states_sell = []\n","        states_buy = []\n","        inventory = []\n","        state = self.get_state(0)\n","        for t in range(0, len(self.test_data) - 1):\n","            action = self.act(state)\n","            next_state = self.get_state(t + 1)\n","            \n","          # Buy\n","            if action == 1 and initial_money >= self.test_data[t] and t < (len(self.test_data) - self.half_window):\n","                inventory.append(self.test_data[t])\n","                initial_money -= self.test_data[t]\n","                states_buy.append(t)\n","                # print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.test_data[t], initial_money))\n","                \n","          # Sell \n","            elif action == 2 and len(inventory):\n","                bought_price = inventory.pop(0)\n","                initial_money += self.test_data[t]\n","                states_sell.append(t)\n","                try:\n","                    reward = ((close[t] - bought_price) / bought_price) * 100\n","                except:\n","                    reward = 0\n","                # print('day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'% (t, close[t], reward, initial_money))\n","            \n","            state = next_state\n","        reward = ((initial_money - starting_money) / starting_money) * 100\n","        total_gains = initial_money - starting_money\n","        return states_buy, states_sell, total_gains, reward"],"metadata":{"id":"5EwDPgDBhwq_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["close = df.Close.values.tolist()\n","close_test = df_test.Close.values.tolist()\n","initial_money = 10000\n","window_size = 5\n","batch_size = 32\n","agent = Agent(state_size = window_size, \n","              window_size = window_size, \n","              trend = close, \n","              test_data = close_test,\n","              batch_size = batch_size)\n","lossV, profitV, profitIter = agent.train(iterations = 2000, checkpoint = 10, initial_money = initial_money)"],"metadata":{"id":"uqqu7_iaiJIY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize = (20,10))\n","plt.plot(lossV, color='r')\n","plt.title('Loss during Training')\n","plt.xlabel('Iterations')\n","plt.ylabel('Loss')\n","plt.show()"],"metadata":{"id":"Q5GW4nIbWMjB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize = (20,10))\n","plt.plot(profitV, color='r')\n","plt.title('Profit during Training (Final Profit: %f)'%(profitV[len(profitV)-1]))\n","plt.xlabel('Iterations')\n","plt.ylabel('Profit')\n","plt.show()"],"metadata":{"id":"7lBG372fZk_C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize = (20,10))\n","plt.plot(profitIter, color='r')\n","plt.title('Profit during Training (Final Profit: %f)'%(profitIter[len(profitIter)-1]))\n","plt.xlabel('Iterations')\n","plt.ylabel('Profit')\n","plt.show()"],"metadata":{"id":"2rNikcSLyGZw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Test"],"metadata":{"id":"B_BzpvvKSzlQ"}},{"cell_type":"code","source":["states_buy, states_sell, total_gains, reward = agent.evaluate(initial_money = initial_money)"],"metadata":{"id":"slDSqNXskNtD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize = (20,10))\n","plt.plot(close_test, color='r', lw=2.)\n","plt.plot(close_test, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n","plt.plot(close_test, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n","plt.title('Testing Close Price (Total Profit: %f)'%(total_gains))\n","plt.legend()\n","plt.show()"],"metadata":{"id":"KZ2Yb2rUkPS5"},"execution_count":null,"outputs":[]}]}